{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 1. \n",
    "\n",
    "## Part 1. Pytorch performance, benchmarking\n",
    "\n",
    "\n",
    "At first, let's connect to our env and import all libraries, as well as check CUDA availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import FloatTensor\n",
    "\n",
    "\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", is_cuda:=torch.cuda.is_available())\n",
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device is {DEVICE}\")\n",
    "\n",
    "if is_cuda:\n",
    "    device_properties = torch.cuda.get_device_properties(0)  # device index\n",
    "    print(f\"Device name: {device_properties.name}\")\n",
    "    print(f\"Device total memory: {device_properties.total_memory / 2 ** 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microbenchmarking\n",
    "\n",
    "To microbenchmark a single function or small portion of the code, you can use `torch.utils.benchmark`. Let's try it out with two examples:\n",
    "- Batched matrix multiply\n",
    "- Multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batched Matmul example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_matmul_at_home(a: FloatTensor, b: FloatTensor) -> FloatTensor:\n",
    "    \"\"\"Batched matmul by hands\n",
    "\n",
    "    Args:\n",
    "        a (FloatTensor) [B, M, N] tensor\n",
    "        b (FloatTensor): [B, N, K] tensor\n",
    "\n",
    "    Returns:\n",
    "        FloatTensor: [B, M, K] tensor - for each element in a batch, computes corresponding\n",
    "        matrix multiplication of MxN and NxK matrices\n",
    "    \"\"\"\n",
    "\n",
    "    # expand dims so that broadcasted multiply works:\n",
    "    # a.unsqueeze(-1): [B, M, N, 1]\n",
    "    # b.unsqueeze(-3): [B, 1, N, K]\n",
    "\n",
    "    # after broadcasting: [B, M, N, K]\n",
    "    prod = a.unsqueeze(-1) * b.unsqueeze(-3)  # a and b are viewed as 4D tensors. We get product across N dimension.\n",
    "\n",
    "    # all we need is to reduce N dimension: sum over it to get [B, M, K]\n",
    "    out = prod.sum(dim=-2)\n",
    "\n",
    "    return out\n",
    "\n",
    "def batched_matmul_torch(a: FloatTensor, b: FloatTensor) -> FloatTensor:\n",
    "    \"\"\"Batched matmul using torch.bmm\n",
    "\n",
    "    Args:\n",
    "        a (FloatTensor) [B, M, N] tensor\n",
    "        b (FloatTensor): [B, N, K] tensor\n",
    "\n",
    "    Returns:\n",
    "        FloatTensor: [B, M, K] tensor - computes each matmul in a batch\n",
    "    \"\"\"\n",
    "    return torch.bmm(a, b)\n",
    "\n",
    "\n",
    "a = torch.randn(6, 128, 256).to(DEVICE)\n",
    "b = torch.randn(6, 256, 512).to(DEVICE)\n",
    "\n",
    "\n",
    "# need to tweak rtol and atol due to numeric precision\n",
    "try:\n",
    "    torch.testing.assert_close(batched_matmul_at_home(a, b), batched_matmul_torch(a, b))\n",
    "    print(\"Default test is OK!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught Assertion error with standard assertion thresholds! Error: {e}\\n\\n*******\\n\")\n",
    "finally:\n",
    "    print(\"Relaxing testing requirements:...\")\n",
    "    torch.testing.assert_close(batched_matmul_at_home(a, b), batched_matmul_torch(a, b), rtol=1e-4, atol=1e-4)\n",
    "    print(\"Relaxed test is OK!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's the problem? The problem lies in __different kernels execution__ - floating-point arithmetic can produce varying results (up to small errors) even if the implementation is correct. For instance, in our case:\n",
    "\n",
    "- `torch.bmm` is implemented in optimized CUBLAS kernels that accumulate in float32 consistently.\n",
    "- Our `batched_matmul_at_home` uses `out = prod.sum(dim=-2)` which accumulates in the same dtype as inputs. With large sums (e.g. 256 terms per dot product), rounding errors can drift by 1e-5â€“1e-4.\n",
    "\n",
    "\n",
    "\n",
    "**NOTE**: numerical precision of floating point operations can very from the impkementations, environments and libraries, and thus can be introduced by the user's code. It also happens due to the different summation order and nondeterministic operations on the hardware side (`torch.use_deterministic_algorithms(True)` won't help sometimes).\n",
    "\n",
    "**NOTE #2**: the relative error is small enough, which frees you from worrying too much about it (unless you work on super-precie calculation for whatever reason)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test their performance using `benchmark.Timer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "\n",
    "?benchmark.Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals_dict = dict(a=a, b=b)\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt=\"batched_matmul_at_home(a, b)\",\n",
    "    setup=\"from __main__ import batched_matmul_at_home\",\n",
    "    globals=globals_dict,\n",
    ")\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt=\"batched_matmul_torch(a, b)\",\n",
    "    setup=\"from __main__ import batched_matmul_torch\",\n",
    "    globals=globals_dict,\n",
    ")\n",
    "\n",
    "print(t0_res:=t0.timeit(100))\n",
    "print(t1_res:=t1.timeit(100))\n",
    "\n",
    "\n",
    "t0_sec,  = t0_res.times\n",
    "t1_sec,  = t1_res.times\n",
    "\n",
    "print(f\"\\n\\n*******\\n\\n===> torch is faster then vanilla implementation by ~{t0_sec/t1_sec:.3f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One more example on numeric precision (inspired by the example from Efficiend-Dl systems [seminar](https://github.com/mryab/efficient-dl-systems/blob/2024/week01_intro/seminar.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nondeterminism_example.py\n",
    "\n",
    "# run it with CUBLAS_WORKSPACE_CONFIG=:4096:8 python nondeterminism_example.py to ensure CUBLAS determinism\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# make matmul comparable (disable TF32 fast path)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "# determinism guard (PyTorch side). For matmul on CUDA you ALSO need the env var above.\n",
    "torch.use_deterministic_algorithms(True, warn_only=False)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "x = torch.randn(2500, 2500)\n",
    "\n",
    "def matrix_polynomial(x):\n",
    "    y = x + 0.5 * x @ x + 3.0 * x @ x @ x + x @ x @ x @ x\n",
    "    return (y).sum().item()\n",
    "\n",
    "print(\"Torch CPU:\", matrix_polynomial(x), sep='\\t')\n",
    "print(\"Torch GPU:\", matrix_polynomial(x.cuda()), sep='\\t')\n",
    "\n",
    "print(\"Numpy: \", matrix_polynomial(x.numpy()), sep='\\t')\n",
    "print(\"Numpy built-in: \", (x.numpy() +\n",
    "                           0.5 * np.linalg.matrix_power(x.numpy(), 2) +\n",
    "                           3.0 * np.linalg.matrix_power(x.numpy(), 3) +\n",
    "                           np.linalg.matrix_power(x.numpy(), 4)\n",
    "                           ).sum(), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUBLAS_WORKSPACE_CONFIG=:4096:8 python nondeterminism_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making your own benchmark tool\n",
    "\n",
    "In python, you can implement your own microbenchmarking, e.g., by using decorators. We need to explicitly enforce GPU-CPU synchronization by placing `torch.cuda.synchronize` -- the interpreter will wait all launched CUDA processes at that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from statistics import mean, stdev\n",
    "\n",
    "    \n",
    "def microbench_this_func(iterations: int = 100):\n",
    "    is_benchmarked = False\n",
    "\n",
    "    def outer_wrapper(func):\n",
    "        \n",
    "        def wrapper(*args, **kwargs):\n",
    "            nonlocal is_benchmarked\n",
    "            name = func.__name__\n",
    "            if not is_benchmarked:\n",
    "                measurements = []\n",
    "                for _ in trange(iterations, desc=f\"Benchmarking {name}\"):\n",
    "                    start = torch.cuda.Event(enable_timing=True)\n",
    "                    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "                    start.record()\n",
    "                    func(*args, **kwargs)\n",
    "                    end.record()\n",
    "                    torch.cuda.synchronize()  # wait the processes\n",
    "    \n",
    "                    measurements.append(start.elapsed_time(end) / 1000)\n",
    "\n",
    "                is_benchmarked = True  # mark the function and not benchmark later\n",
    "\n",
    "                # summarize the report:\n",
    "                mean_time = mean(measurements)\n",
    "                std_time = stdev(measurements)\n",
    "                print(f'Function {func} took {mean_time:.5f}+-{std_time:.5f}s')\n",
    "\n",
    "            res = func(*args, **kwargs)\n",
    "            return res\n",
    "        return wrapper    \n",
    "    return outer_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@microbench_this_func(100)\n",
    "def batched_matmul_torch(a: FloatTensor, b: FloatTensor) -> FloatTensor:\n",
    "    \"\"\"Batched matmul using torch.bmm\n",
    "\n",
    "    Args:\n",
    "        a (FloatTensor) [B, M, N] tensor\n",
    "        b (FloatTensor): [B, N, K] tensor\n",
    "\n",
    "    Returns:\n",
    "        FloatTensor: [B, M, K] tensor - computes each matmul in a batch\n",
    "    \"\"\"\n",
    "    return torch.bmm(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batched_matmul_torch(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multihead Attention example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha_looped(q: FloatTensor, k: FloatTensor, v: FloatTensor, n_heads: int):\n",
    "    \"\"\"\n",
    "    q, k, v --> [B, N, D] tensors\n",
    "    n_heads - number of heads in attention layer\n",
    "\n",
    "    outputs: Attention output\n",
    "    computation for multiple attention heads is performed in for-loop\n",
    "\n",
    "    \"\"\"\n",
    "    B, N, D = q.shape\n",
    "    assert D % n_heads == 0, \"D must be divisible by n_heads\"\n",
    "    d_head = D // n_heads\n",
    "    scale = 1.0 / d_head ** 0.5\n",
    "\n",
    "    # Split last dim into heads: list of [B, N, d_head]\n",
    "    qs = q.split(d_head, -1)  # method for splitting the tensor across the ast dimension with specified slice size (`d_head`)\n",
    "    ks = k.split(d_head, -1)\n",
    "    vs = v.split(d_head, -1)\n",
    "\n",
    "    head_outputs = []\n",
    "    for qi, ki, vi in zip(qs, ks, vs):\n",
    "        # [B, N, d_head] @ [B, d_head, N] -> [B, N, N]\n",
    "        scores = torch.matmul(qi, ki.transpose(-2, -1)) * scale\n",
    "        attn = scores.softmax(-1)\n",
    "        # [B, N, N] @ [B, N, d_head] -> [B, N, d_head]\n",
    "        out_i = torch.matmul(attn, vi)\n",
    "        head_outputs.append(out_i)\n",
    "\n",
    "    # Concatenate heads back: [B, N, D]\n",
    "    out = torch.cat(head_outputs, dim=-1)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def mha_batched(q: FloatTensor, k: FloatTensor, v: FloatTensor, n_heads: int):\n",
    "    \"\"\"\n",
    "    q, k, v --> [B, N, D] tensors\n",
    "    n_heads - number of heads in attention layer\n",
    "\n",
    "    outputs: Attention output\n",
    "    computation for multiple attention heads is performed more efficiently iwith matched matmul\n",
    "\n",
    "    \"\"\"\n",
    "    B, N, D = q.shape\n",
    "    assert D % n_heads == 0, \"D must be divisible by n_heads\"\n",
    "    d_head = D // n_heads\n",
    "    scale = 1.0 / d_head ** 0.5\n",
    "\n",
    "    # Reshape to [B, n_heads, N, d_head]\n",
    "    qh = q.view(B, N, n_heads, d_head).transpose(1, 2)  # [B, H, N, d_head]\n",
    "    kh = k.view(B, N, n_heads, d_head).transpose(1, 2)  # [B, H, N, d_head]\n",
    "    vh = v.view(B, N, n_heads, d_head).transpose(1, 2)  # [B, H, N, d_head]\n",
    "\n",
    "    # Attention scores: [B, H, N, N]\n",
    "    scores = torch.matmul(qh, kh.transpose(-2, -1)) * scale\n",
    "    attn = scores.softmax(dim=-1)\n",
    "\n",
    "    # Weighted sum: [B, H, N, d_head]\n",
    "    out_heads = torch.matmul(attn, vh)\n",
    "\n",
    "    # Merge heads back to [B, N, D]\n",
    "    out = out_heads.transpose(1, 2).contiguous().view(B, N, D)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def mha_sdpa_reference(q, k, v, n_heads, *, is_causal=False):\n",
    "    \"\"\"\n",
    "    Reference using torch.nn.functional.scaled_dot_product_attention\n",
    "    Shapes expected by SDPA: (*, L, E), (*, S, E).\n",
    "    We'll pass (B,H,N,d) so it returns (B,H,N,d).\n",
    "    \"\"\"\n",
    "    B, N, D = q.shape\n",
    "    d_head = D // n_heads\n",
    "    # [B,H,N,d]\n",
    "    qh = q.view(B, N, n_heads, d_head).transpose(1, 2)\n",
    "    kh = k.view(B, N, n_heads, d_head).transpose(1, 2)\n",
    "    vh = v.view(B, N, n_heads, d_head).transpose(1, 2)\n",
    "    # Let SDPA choose the scale (it uses 1/sqrt(d_head) if scale=None)\n",
    "    oh = F.scaled_dot_product_attention(qh, kh, vh,\n",
    "                                        attn_mask=None,\n",
    "                                        dropout_p=0.0,\n",
    "                                        is_causal=is_causal,\n",
    "                                        scale=None)           # [B,H,N,d]\n",
    "    return oh.transpose(1, 2).contiguous().view(B, N, D)      # [B,N,D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn((16, 64, 256)).to(DEVICE)\n",
    "k = torch.randn((16, 64, 256)).to(DEVICE)\n",
    "v = torch.randn((16, 64, 256)).to(DEVICE)\n",
    "\n",
    "n_heads = 8\n",
    "\n",
    "torch.testing.assert_close(mha_looped(q, k, v, n_heads), mha_batched(q, k, v, n_heads))\n",
    "torch.testing.assert_close(mha_looped(q, k, v, n_heads), mha_sdpa_reference(q, k, v, n_heads))\n",
    "torch.testing.assert_close(mha_batched(q, k, v, n_heads), mha_sdpa_reference(q, k, v, n_heads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals_dict_mha = dict(q=q, k=k, v=v, n_heads=n_heads)\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt=\"mha_looped(q, k, v, n_heads)\",\n",
    "    setup=\"from __main__ import mha_looped\",\n",
    "    globals=globals_dict_mha,\n",
    ")\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt=\"mha_batched(q, k, v, n_heads)\",\n",
    "    setup=\"from __main__ import mha_batched\",\n",
    "    globals=globals_dict_mha,\n",
    ")\n",
    "\n",
    "t2 = benchmark.Timer(\n",
    "    stmt=\"mha_sdpa_reference(q, k, v, n_heads)\",\n",
    "    setup=\"from __main__ import mha_sdpa_reference\",\n",
    "    globals=globals_dict_mha,\n",
    ")\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))\n",
    "print(t2.timeit(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling code\n",
    "\n",
    "Here, we will see how we can profile the whole execution of the program. It covers mroe general task compared to microbenchmarking - you can identify bottlnecks of your program and verify your solutions (whether they work or not)\n",
    "\n",
    "\n",
    "\n",
    "We will use `bench_attention.py` as an example of DL pipeline we want to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.compile`\n",
    "\n",
    "TLDR: it detects the overhead introduced by the model, interferes with the computational graph and rewrites the parts of it. Let's try it out!\n",
    "\n",
    "\n",
    "Further reading:\n",
    "- [Ways to use `torch.compile`](https://blog.ezyang.com/2024/11/ways-to-use-torch-compile/)\n",
    "- [`torch.compile`, the missing manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example - fuse elementwise operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi, sqrt\n",
    "\n",
    "@microbench_this_func()\n",
    "def foo(x, y):\n",
    "    a = torch.sin(x) + (x + y)\n",
    "    b = torch.cos(y) + (y + x)\n",
    "    c = a + b\n",
    "    gelu = 0.5 * x * (1 + torch.tanh(sqrt(2 / pi) * (c + 0.044715 * c @ c @ c)))\n",
    "    gelu2 = 0.5 * x * (1 + torch.tanh(sqrt(2 / torch.acos(torch.tensor(-1))) * (c + 0.044715 * c @ c @ c)))\n",
    "    return gelu + gelu2\n",
    "\n",
    "\n",
    "@microbench_this_func()\n",
    "@torch.compile\n",
    "def foo_jit_compiled(x, y):\n",
    "    return foo(x, y)\n",
    "\n",
    "a = torch.randn(1000, 1000).cuda()\n",
    "b = torch.randn(1000, 1000).cuda()\n",
    "x = foo(a, b)\n",
    "y = foo_jit_compiled(a, b)\n",
    "\n",
    "\n",
    "torch.testing.assert_close(x, y, atol=5e-4, rtol=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@microbench_this_func()\n",
    "def compiled(x, y):\n",
    "    return foo_jit_compiled(x, y)\n",
    "\n",
    "_ = compiled(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fuse it, we eradicate data transfer needed in the eager pytorch mode --> more FLOPs --> faster execution\n",
    "\n",
    "Let's try with toy DL example (adapted from original tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import densenet121\n",
    "\n",
    "def simple_timer(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(b):\n",
    "    return (\n",
    "        torch.randn(b, 3, 128, 128).to(torch.float32).cuda(),\n",
    "        torch.randint(1000, (b,)).cuda(),\n",
    "    )\n",
    "\n",
    "N_ITERS = 10\n",
    "\n",
    "def init_model():\n",
    "    return densenet121().to(torch.float32).cuda()\n",
    "\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "# Reset since we are using a different mode.\n",
    "import torch._dynamo\n",
    "torch._dynamo.reset()\n",
    "\n",
    "model_opt = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "inp = generate_data(16)[0]\n",
    "with torch.no_grad():\n",
    "    print(f\"Eager execution took {simple_timer(lambda: model(inp))[1]:.3f} sec\")\n",
    "    print(f\"Compilation + execution took {simple_timer(lambda: model_opt(inp))[1]:.3f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiled version takes longer to execute due to the compilation of the optimized kernels. If we run the optimized model several times, the result will be faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)[0]\n",
    "    with torch.no_grad():\n",
    "        _, eager_time = simple_timer(lambda: model(inp))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager eval time {i}: {eager_time:.4f} sec\")\n",
    "\n",
    "print(\"~\" * 10)\n",
    "\n",
    "compile_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)[0]\n",
    "    with torch.no_grad():\n",
    "        _, compile_time = simple_timer(lambda: model_opt(inp))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile eval time {i}: {compile_time:.4f} sec\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "import numpy as np\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert(speedup > 1)\n",
    "print(f\"(eval) eager median: {eager_med:.4f} sec, compile median: {compile_med:.4f} sec, speedup: {speedup:.3f}x\")\n",
    "print(\"~\" * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def train(mod, data):\n",
    "    opt.zero_grad(True)\n",
    "    pred = mod(data[0])\n",
    "    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "eager_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)\n",
    "    _, eager_time = simple_timer(lambda: train(model, inp))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager train time {i}: {eager_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "model = init_model()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "train_opt = torch.compile(train, mode=\"reduce-overhead\")\n",
    "\n",
    "compile_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(16)\n",
    "    _, compile_time = simple_timer(lambda: train_opt(model, inp))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile train time {i}: {compile_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert(speedup > 1)\n",
    "print(f\"(train) eager median: {eager_med:.4f} sec, compile median: {compile_med:.4f} sec, speedup: {speedup:.3f}x\")\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free practise :)\n",
    "\n",
    "_A seminarist might ask it :)_\n",
    "\n",
    "- Profile execution of batched matmul implementations and multihead attention implementations with `torch.profiler.profile`:\n",
    "  - Obtain traces, analyze them\n",
    "  - Try to find the difference in traces and verify the claim we made regarding the kernels\n",
    "- Improve `microbench_this_func` decorator:\n",
    "  - It should handle arguments too - if the function is called with different shapes, the benchmark should be done too\n",
    "  - The microbench should store the measurements rather than just printing (can be useful for automatic reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
