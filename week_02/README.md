# On Transformers and Bitter Lesson

Lecturer and seminarian: [Ivan Rubachev](https://www.hse.ru/org/persons/190912012/)

Recording (in Russian): [lecture and seminar](https://disk.yandex.ru/d/mzXlT0U3MzEZkQ/%D0%9F%D0%9C%D0%98/DL%202/lecture%2Bseminar_02.mp4).

## Annotation
In this talk we’ll dive into the landscape of model architectures in deep learning, with a focus on the world around transformers. We’ll briefly recall what a transformer is, trace the evolution from encoder–decoder to encoder-only and decoder-only models, and touch on the rise of “efficient mixers” such as state space models, linear attention, and beyond. We’ll conclude by reflecting on the role of data and compute. The lecture is inspired in part by [The Bitter Lesson](http://incompleteideas.net/IncIdeas/BitterLesson.html) and blends a brain dump with some entertaining insights from recent years of architectural exploration.