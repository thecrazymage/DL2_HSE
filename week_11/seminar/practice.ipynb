{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ad53c2",
   "metadata": {},
   "source": [
    "# Семинар: Нейросетевое ранжирование\n",
    "### Семинарист: Матвеев Артем, Yandex\n",
    "\n",
    "В этом семинаре мы познакомимся с методами кодирование входных признаков для моделей нейросетевого ранжирвания: Mutisize Encoding with Unified Embeddings для категориальных признаков и Picewise Linear Encoding для вещественных. А также имплементируем DCNv2 и проверим всю схему на наборе данных Yambda. Попробуем обогнать такой сильный бейзлайн на табличных данных как Catboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b770a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O data/pool.parquet https://huggingface.co/datasets/matfu21/yambda-50m-features/blob/main/pool.parquet\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0024473",
   "metadata": {},
   "source": [
    "## 1. Yambda - 50m\n",
    "\n",
    "Набор данных Yambda-5B — это большой открытый датасет, содержащий 4,79 млрд взаимодействий пользователь–трек, собранных от 1 млн пользователей и охватывающих 9,39 млн треков. Набор включает как неявный отклик (например, факты прослушивания), так и явный отклик в виде лайков и дизлайков. Кроме того, в нём есть отдельные метки для органических и рекомендательных взаимодействий, а также предвычисленные аудио-эмбеддинги, что упрощает разработку контент-ориентированных рекомендательных систем. Данные собрны с Яндекс Музыки. \n",
    "\n",
    "Ссылки: \n",
    "- hugging-face: https://huggingface.co/datasets/yandex/yambda\n",
    "- hugging-face: https://huggingface.co/datasets/matfu21/yambda-50m-features\n",
    "- arxiv: https://arxiv.org/pdf/2505.22238\n",
    "- argus arxiv: https://www.arxiv.org/pdf/2507.15994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72664025",
   "metadata": {},
   "source": [
    "#### Метка класса:\n",
    "\n",
    "`target_full_play`\n",
    "- 1 — если событие listen и трек был прослушан ≥ 95%.\n",
    "- 0 — если событие listen, но трек не дотянул до 95%.\n",
    "\n",
    "#### Категориальные признаки\n",
    "\n",
    "`uid` - ID пользователя.  \n",
    "`item_id` - ID трека.\n",
    "\n",
    "#### Вещественные признаки\n",
    "\n",
    "`timestamp` - Время события в секундах (квантовано по 5 секунд, но мы храним в секундах).\n",
    "\n",
    "1. Пользовательские (user_hist_*)\n",
    "\n",
    "`user_hist_radio_skip_fraction_before`  \n",
    "`user_hist_skip_fraction_before`  \n",
    "`user_hist_radio_skip_before`  \n",
    "`user_hist_like_before`  \n",
    "`user_hist_track_finished_before`  \n",
    "`user_hist_last_ts_before`  \n",
    "`user_hist_radio_play_fraction_before`  \n",
    "`user_hist_skip_frequency_before`  \n",
    "`user_hist_skip_before`  \n",
    "\n",
    "2. Посессионные признаки (session_hist_*)\n",
    "\n",
    "`session_hist_duration_seconds_before`  \n",
    "`session_hist_skip_fraction_before`  \n",
    "`session_hist_played_time_seconds_before`  \n",
    "`session_hist_skip_before`  \n",
    "`session_hist_events_before`  \n",
    "`session_hist_plays_before`  \n",
    "`session_start_ts`  \n",
    "\n",
    "3. item-ые (item_hist_*)\n",
    "\n",
    "`item_hist_avg_played_ratio_before`  \n",
    "`item_hist_track_finished_before`  \n",
    "\n",
    "4. Кросс-признаки (user_item_hist_*)\n",
    "\n",
    "`user_item_hist_track_finished_before`  \n",
    "`user_item_hist_last_ts_before`  \n",
    "`user_item_hist_avg_played_ratio_before`  \n",
    "`user_item_hist_time_since_last_event_seconds`  \n",
    "`user_item_hist_time_span_seconds_before`  \n",
    "`user_item_hist_skip_before`  \n",
    "`user_item_hist_plays_before`  \n",
    "`user_item_hist_like_before`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72fe0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import typing as tp\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e6760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YambdaDatasetUtils:\n",
    "    NUM_COLS = [\n",
    "        # timestamp\n",
    "        \"timestamp\",\n",
    "\n",
    "        # user history\n",
    "        \"user_hist_radio_skip_fraction_before\",\n",
    "        \"user_hist_skip_fraction_before\",\n",
    "        \"user_hist_radio_skip_before\",\n",
    "        \"user_hist_like_before\",\n",
    "        \"user_hist_track_finished_before\",\n",
    "        \"user_hist_last_ts_before\",\n",
    "        \"user_hist_radio_play_fraction_before\",\n",
    "        \"user_hist_skip_frequency_before\",\n",
    "        \"user_hist_skip_before\",\n",
    "\n",
    "        # session-level\n",
    "        \"session_hist_duration_seconds_before\",\n",
    "        \"session_hist_skip_fraction_before\",\n",
    "        \"session_hist_played_time_seconds_before\",\n",
    "        \"session_hist_skip_before\",\n",
    "        \"session_hist_events_before\",\n",
    "        \"session_hist_plays_before\",\n",
    "        \"session_start_ts\",\n",
    "\n",
    "        # item-level\n",
    "        \"item_hist_avg_played_ratio_before\",\n",
    "        \"item_hist_track_finished_before\",\n",
    "\n",
    "        # user × item cross-features\n",
    "        \"user_item_hist_track_finished_before\",\n",
    "        \"user_item_hist_last_ts_before\",\n",
    "        \"user_item_hist_avg_played_ratio_before\",\n",
    "        \"user_item_hist_time_since_last_event_seconds\",\n",
    "        \"user_item_hist_time_span_seconds_before\",\n",
    "        \"user_item_hist_skip_before\",\n",
    "        \"user_item_hist_plays_before\",\n",
    "        \"user_item_hist_like_before\",\n",
    "    ]\n",
    "    CAT_COLS = [\n",
    "        \"uid\",\n",
    "        \"item_id\",\n",
    "    ]\n",
    "    LABEL_COl = \"target_full_play\"\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_dense_features(cls, lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Preprocess dense features:\n",
    "        - Fill missing values with 0\n",
    "        - Apply log transformation: log(x + 1)\n",
    "        \"\"\"\n",
    "        expressions = []\n",
    "        for col in cls.NUM_COLS:\n",
    "            if col == \"timestamp\": # timestamp has uniform distribution, we don't need to apply log transformation\n",
    "                continue\n",
    "            expressions.append(\n",
    "                pl.col(col).fill_null(0).add(1).log()\n",
    "            )\n",
    "        lf = lf.with_columns(expressions)\n",
    "        return lf\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_categorical_features(cls, lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Preprocess categorical features:\n",
    "        - Fill missing values with 0\n",
    "        \"\"\"\n",
    "        expressions = []\n",
    "        for col in cls.CAT_COLS:\n",
    "            expressions.append(\n",
    "                pl.col(col).fill_null(0)\n",
    "            )\n",
    "        lf = lf.with_columns(expressions)\n",
    "        return lf\n",
    "\n",
    "    @classmethod\n",
    "    def split(cls, lf: pl.DataFrame, threshold: int) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split data into train and test sets by threshold\n",
    "        \"\"\"\n",
    "        df_train = lf.filter(pl.col(\"timestamp\") <= threshold)\n",
    "        df_test  = lf.filter(pl.col(\"timestamp\")  > threshold)\n",
    "        return df_train, df_test\n",
    "\n",
    "    @classmethod\n",
    "    def read_preprocess_and_split(cls, path: str, valid_period_in_sec: int) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        path: path to parquet file\n",
    "        valid_period_in_sec: valid period in seconds\n",
    "        \"\"\"\n",
    "        lf = pl.scan_parquet(path)\n",
    "        lf = cls.preprocess_categorical_features(lf)\n",
    "        lf = cls.preprocess_dense_features(lf)\n",
    "        df = lf.collect()\n",
    "\n",
    "        max_timestamp = df.select(pl.col(\"timestamp\").max()).item()\n",
    "        threshold = max_timestamp - valid_period_in_sec\n",
    "        df_train, df_test = cls.split(df, threshold)\n",
    "\n",
    "        return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fe1dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19610232, 30), (2853323, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS_PATH = './data/pool.parquet'\n",
    "MONTH_PERIOD = 4 * 7 * 24 * 60 * 60  # 4 weeks, 7 days, 24 hours, 60 minutes, 60 seconds\n",
    "df_train, df_test = YambdaDatasetUtils.read_preprocess_and_split(\n",
    "    path=DATASETS_PATH,\n",
    "    valid_period_in_sec=MONTH_PERIOD,\n",
    ")\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d05faa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 30)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target_full_play</th><th>uid</th><th>item_id</th><th>timestamp</th><th>user_hist_radio_skip_fraction_before</th><th>user_hist_skip_fraction_before</th><th>user_hist_radio_skip_before</th><th>user_hist_like_before</th><th>user_hist_track_finished_before</th><th>user_hist_last_ts_before</th><th>user_hist_radio_play_fraction_before</th><th>user_hist_skip_frequency_before</th><th>user_hist_skip_before</th><th>session_hist_duration_seconds_before</th><th>session_hist_skip_fraction_before</th><th>session_hist_played_time_seconds_before</th><th>session_hist_skip_before</th><th>session_hist_events_before</th><th>session_hist_plays_before</th><th>session_start_ts</th><th>item_hist_avg_played_ratio_before</th><th>item_hist_track_finished_before</th><th>user_item_hist_track_finished_before</th><th>user_item_hist_last_ts_before</th><th>user_item_hist_avg_played_ratio_before</th><th>user_item_hist_time_since_last_event_seconds</th><th>user_item_hist_time_span_seconds_before</th><th>user_item_hist_skip_before</th><th>user_item_hist_plays_before</th><th>user_item_hist_like_before</th></tr><tr><td>bool</td><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>true</td><td>100</td><td>1441281</td><td>39420</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.693147</td><td>10.582054</td><td>0.693147</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>10.582054</td><td>4.241981</td><td>2.70805</td><td>0.693147</td><td>0.0</td><td>4.615121</td><td>10.582054</td><td>22.176245</td><td>0.0</td><td>0.693147</td><td>0.0</td></tr><tr><td>true</td><td>100</td><td>8326270</td><td>39420</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.663439</td><td>0.0</td><td>0.693147</td><td>0.693147</td><td>10.582054</td><td>4.546835</td><td>0.693147</td><td>0.0</td><td>0.0</td><td>3.091042</td><td>10.582054</td><td>22.180415</td><td>0.693147</td><td>0.693147</td><td>0.0</td></tr><tr><td>true</td><td>100</td><td>286361</td><td>39625</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.098612</td><td>10.582054</td><td>0.693147</td><td>0.0</td><td>0.0</td><td>5.327876</td><td>0.0</td><td>5.620401</td><td>0.0</td><td>1.098612</td><td>1.098612</td><td>10.582054</td><td>4.615121</td><td>1.098612</td><td>0.693147</td><td>0.0</td><td>3.931826</td><td>10.587241</td><td>22.179239</td><td>1.098612</td><td>1.386294</td><td>0.0</td></tr><tr><td>true</td><td>100</td><td>732449</td><td>40110</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.386294</td><td>10.587241</td><td>0.693147</td><td>0.0</td><td>0.0</td><td>6.53814</td><td>0.0</td><td>6.133398</td><td>0.0</td><td>1.386294</td><td>1.386294</td><td>10.582054</td><td>4.219508</td><td>1.098612</td><td>1.386294</td><td>0.0</td><td>4.251348</td><td>10.599406</td><td>22.1807</td><td>1.098612</td><td>1.791759</td><td>0.0</td></tr><tr><td>false</td><td>100</td><td>3397170</td><td>40360</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.609438</td><td>10.599406</td><td>0.693147</td><td>0.0</td><td>0.0</td><td>6.846943</td><td>0.0</td><td>6.552508</td><td>0.0</td><td>1.609438</td><td>1.609438</td><td>10.582054</td><td>4.433789</td><td>1.386294</td><td>0.0</td><td>0.0</td><td>0.0</td><td>10.605619</td><td>22.175255</td><td>0.693147</td><td>0.693147</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 30)\n",
       "┌─────────────┬─────┬─────────┬───────────┬───┬─────────────┬────────────┬────────────┬────────────┐\n",
       "│ target_full ┆ uid ┆ item_id ┆ timestamp ┆ … ┆ user_item_h ┆ user_item_ ┆ user_item_ ┆ user_item_ │\n",
       "│ _play       ┆ --- ┆ ---     ┆ ---       ┆   ┆ ist_time_sp ┆ hist_skip_ ┆ hist_plays ┆ hist_like_ │\n",
       "│ ---         ┆ u32 ┆ u32     ┆ u32       ┆   ┆ an_secon…   ┆ before     ┆ _before    ┆ before     │\n",
       "│ bool        ┆     ┆         ┆           ┆   ┆ ---         ┆ ---        ┆ ---        ┆ ---        │\n",
       "│             ┆     ┆         ┆           ┆   ┆ f64         ┆ f64        ┆ f64        ┆ f64        │\n",
       "╞═════════════╪═════╪═════════╪═══════════╪═══╪═════════════╪════════════╪════════════╪════════════╡\n",
       "│ true        ┆ 100 ┆ 1441281 ┆ 39420     ┆ … ┆ 22.176245   ┆ 0.0        ┆ 0.693147   ┆ 0.0        │\n",
       "│ true        ┆ 100 ┆ 8326270 ┆ 39420     ┆ … ┆ 22.180415   ┆ 0.693147   ┆ 0.693147   ┆ 0.0        │\n",
       "│ true        ┆ 100 ┆ 286361  ┆ 39625     ┆ … ┆ 22.179239   ┆ 1.098612   ┆ 1.386294   ┆ 0.0        │\n",
       "│ true        ┆ 100 ┆ 732449  ┆ 40110     ┆ … ┆ 22.1807     ┆ 1.098612   ┆ 1.791759   ┆ 0.0        │\n",
       "│ false       ┆ 100 ┆ 3397170 ┆ 40360     ┆ … ┆ 22.175255   ┆ 0.693147   ┆ 0.693147   ┆ 0.0        │\n",
       "└─────────────┴─────┴─────────┴───────────┴───┴─────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10203913",
   "metadata": {},
   "source": [
    "Проверяем, что пропуски в данных отсуствуют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_train.null_count().pipe(sum).item() == 0\n",
    "assert df_test.null_count().pipe(sum).item() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33604186",
   "metadata": {},
   "source": [
    "Смотрим на количество уникальных значений категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16cafab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': 452784, 'uid': 8410}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = {col: df_train[col].n_unique() for col in YambdaDatasetUtils.CAT_COLS}\n",
    "sorted_unique_counts = dict(\n",
    "    sorted(unique_counts.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "sorted_unique_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896add6",
   "metadata": {},
   "source": [
    "Посчитаем необходимое количество GPU памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f5ebb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7593154907226562 GB\n"
     ]
    }
   ],
   "source": [
    "uniq_ids = sum(sorted_unique_counts.values())\n",
    "embedding_dim = 256\n",
    "bytes_in_float = 4\n",
    "mult = 4  # params + grads + moment1 + moment2\n",
    "bytes_in_gb = 1024 * 1024 * 1024\n",
    "print(f'{uniq_ids * embedding_dim * bytes_in_float * mult / bytes_in_gb} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593e496",
   "metadata": {},
   "source": [
    "Это всего лишь Yambda-50m, реальные размеры это Yambda-1e12 =). Даже если возьмем Yambda-5B, это будет 1,000,000 уникальный uid и 9,390,623 уникальный item_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b82a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.637081146240234 GB\n"
     ]
    }
   ],
   "source": [
    "uniq_ids = 1_000_000 + 9_390_623\n",
    "print(f'{uniq_ids * embedding_dim * bytes_in_float * mult / bytes_in_gb} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee1aed",
   "metadata": {},
   "source": [
    "Нужен более масштабируемый подход."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1c190",
   "metadata": {},
   "source": [
    "## 2. Multisize Unified Embeddings\n",
    "\n",
    "Для кодирования категориальных признаков будем использовать Multisize Unified кодирование от Google DeepMind: [Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems](https://arxiv.org/abs/2305.12102).\n",
    "\n",
    "### Общая задача\n",
    "\n",
    "Дан $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_{|D|}, y_{|D|})\\}$ с примерами из $T$ категориальных признаков с словарями $\\{V_1, V_2, \\ldots, V_T\\}$. Каждый пример $x = [v_1, v_2, \\ldots, v_T]$, где $v_i \\in V_i$.\n",
    "\n",
    "- Матрица эмбеддингов $\\mathbf{E} \\in \\mathbb{R}^{M \\times d}$, отображения примера в эмбеддинг $g(\\mathbf{x}; \\mathbf{E})$. \n",
    "- Хеш-функция $h(v) : V \\rightarrow [M]$ назначает значение признака индексу строки (используется в $g(\\mathbf{x}; \\mathbf{E})$).\n",
    "- Функция модели $f(\\mathbf{e}; \\boldsymbol{\\theta})$ преобразует вложения в предсказание.\n",
    "\n",
    "Задача обучения:\n",
    "$$\\arg \\min_{\\mathbf{E}, \\boldsymbol{\\theta}} \\mathcal{L}_D(\\mathbf{E}, \\boldsymbol{\\theta}), \\quad \\text{где} \\quad \\mathcal{L}_D(\\mathbf{E}, \\boldsymbol{\\theta}) = \\sum_{(\\mathbf{x},y) \\in D} \\ell(f(g(\\mathbf{x}; \\mathbf{E}); \\boldsymbol{\\theta}), y).$$\n",
    "\n",
    "Используем $h_t(v)$ для каждого признака $t \\in [T]$. Обозначаем $\\mathbf{e}_m$ для $m$-й строки $\\mathbf{E}$, и $\\mathbb{1}_{u,v}$ как индикатор коллизии хешей между $u$ и $v$.\n",
    "\n",
    "### Как это работает\n",
    "\n",
    "Далее будем предполагать, что $|T|$ = 2.\n",
    "\n",
    "<div style=\"width:90%; margin: auto;\">\n",
    "\n",
    "![](https://i.ibb.co/GKMKcvm/unified-embeddings.png)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df63c05",
   "metadata": {},
   "source": [
    "### Почему это работает (интуиция)\n",
    "\n",
    "Рассмотрим частный случай (решаем бинарную классфикацию с помощью логистической регрессии):\n",
    "\n",
    "$$y_i \\in \\{0, 1\\}$$\n",
    "$$ D_0 = \\{(x_i, y_i) \\in D : y_i = 0\\} $$\n",
    "$$ D_1 = \\{(x_i, y_i) \\in D : y_i = 1\\} $$\n",
    "$$ C_{u,v,0} = |\\{([u, v], y) \\in D : y = 0\\}|$$\n",
    "$$ \\sigma_\\theta(z) = \\frac{1}{1 + \\exp(-\\langle z, \\theta \\rangle)} $$\n",
    "$$ z = g(x; \\mathbf{E}) = [e_{h_1(x_1)}, e_{h_2(x_2)}] $$\n",
    "$$ \\theta = [\\theta_1, \\theta_2],~\\theta_t \\in \\mathbb{R}^M$$\n",
    "\n",
    "\n",
    "Функция потерь бинарной кросс-энтропии:\n",
    "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{(x,y)\\in D_0} \\log \\left( \\frac{1}{1 + \\exp(-\\langle \\theta, g(x; \\mathbf{E}) \\rangle)} \\right) - \\sum_{(x,y)\\in D_1} \\log \\left( \\frac{1}{1 + \\exp(\\langle \\theta, g(x; \\mathbf{E}) \\rangle)} \\right) $$\n",
    "\n",
    "Перепишем функция потерь (через частоты совместного появления):\n",
    "$$ e_{u,v} = [e_{h_1(u)}, e_{h_2(v)}] $$\n",
    "\n",
    "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{u\\in V_1} \\sum_{v\\in V_2} C_{u,v,0} \\log \\left( \\frac{1}{1 + \\exp(-\\theta^\\top e_{u,v})} \\right) + C_{u,v,1} \\log \\left( \\frac{1}{1 + \\exp(\\theta^\\top e_{u,v})} \\right) $$\n",
    "\n",
    "После объединения сигмоидных функций:\n",
    "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{u\\in V_1} \\sum_{v\\in V_2} C_{u,v,0} \\log \\exp(\\theta^\\top e_{u,v}) - (C_{u,v,0} + C_{u,v,1}) \\log(1 + \\exp(\\theta^\\top e_{u,v})) $$\n",
    "\n",
    "Далее будем предполагать, что обучаем наш алгоритм с SGD. Посчитаем градиенты по эмбеддингам. Полный градиент для эмбеддинга с учетом внутри- и межпризнаковых взаимодействий:\n",
    "$$ \\nabla_{E_{h(u)}} \\mathcal{L}_D(\\mathbf{E}, \\theta) = $$\n",
    "$$ \\theta_1 \\sum_{v\\in V_2} C_{u,v,0} - (C_{u,v,0} + C_{u,v,1})\\sigma_\\theta(e_{u,v}) \\tag{1}$$\n",
    "$$ + \\theta_1 \\sum_{w\\in V_1, w\\neq u} \\mathbb{1}_{u,w} \\sum_{v\\in V_2} C_{w,v,0} - (C_{w,v,0} + C_{w,v,1})\\sigma_\\theta(e_{u,v}) \\tag{2}$$\n",
    "$$ + \\theta_2 \\sum_{v\\in V_2} \\mathbb{1}_{u,v} \\sum_{w\\in V_1} C_{w,v,0} - (C_{w,v,0} + C_{w,v,1})\\sigma_\\theta(e_{w,u}) \\tag{3}$$\n",
    "\n",
    "Анализируем:\n",
    "- $(1)$ collisionless компонента.\n",
    "- $(2)$ intra-feature компонента.\n",
    "- $(3)$ inter-feature компонента.\n",
    "- Компоненты $(2)$ и $(3)$ смещают реальный градиент.\n",
    "- Intra-feature bias сонаправлен с collisionless компонентой, поэтому модель не может убрать это смещение.\n",
    "- В случае SGD inter-feature bias может невелирован за счет $\\theta_1$ ортогонально $\\theta_2$, т.к. во время SGD, $e_{h(u)}$ представляет собой линейную комбинацию градиентов по шагам обучения, что означает, что $e_{h(u)}$ может быть разложено на компоненты в направлении $\\theta_1$ и inter-feature компоненты в направлении $\\theta_2$. Поскольку $\\langle\\theta_1, \\theta_2\\rangle = 0$, проекция $\\theta_1^\\top e_{h(u)}$ эффективно устраняет inter-feature  компонент.\n",
    "\n",
    "<div style=\"width:70%; margin: auto;\">\n",
    "\n",
    "![](https://i.ibb.co/xt6nbrZ3/theory-unified.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "Вывод: \n",
    "\n",
    "Не все коллизии одинаково проблематичны. Межпризнаковые коллизии могут быть смягчены однослойной нейронной сетью, поскольку разные признаки обрабатываются разными параметрами модели. Коллизии в рамках одного признака убираем за счет нескольких взятий хешей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f6d95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultihashTransform:\n",
    "    \"\"\"\n",
    "    Applys transformation to training sample\n",
    "    \"\"\"\n",
    "    def __init__(self, cardinality, seeds=None, name='sparse'):\n",
    "        assert seeds is not None\n",
    "        self._cardinality = cardinality\n",
    "        self._name = name\n",
    "        self._seeds = torch.tensor(seeds)\n",
    "\n",
    "    def __call__(self, sample: dict[str, tp.Any]) -> dict[str, tp.Any]:\n",
    "        sample[self._name] = (\n",
    "            (sample[self._name].unsqueeze(2) + self._seeds) % self._cardinality\n",
    "        ).long().flatten(-2)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94372f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n",
      "torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "seeds = [\n",
    "    [2342 + 13 * i, 7777 + 17 * i]\n",
    "    for i in range(len(YambdaDatasetUtils.CAT_COLS))\n",
    "]\n",
    "transform = MultihashTransform(10, seeds)\n",
    "input = {\n",
    "    \"label\": torch.ones((batch_size,)),\n",
    "    \"dense\": torch.randn((batch_size, len(YambdaDatasetUtils.NUM_COLS))),\n",
    "    \"sparse\": torch.arange(len(YambdaDatasetUtils.CAT_COLS)).unsqueeze(0).repeat(batch_size, 1)\n",
    "}\n",
    "print(input[\"sparse\"].shape)\n",
    "output = transform(input)\n",
    "print(output[\"sparse\"].shape)\n",
    "assert output[\"sparse\"].shape == (batch_size, 2 * len(YambdaDatasetUtils.CAT_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99933755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedEmbeddings(nn.Module):\n",
    "    def __init__(self, cardinality, embedding_dim):\n",
    "        super().__init__()\n",
    "        self._cardinality = cardinality\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=cardinality, embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, ids: torch.Tensor):\n",
    "        # ids shape: [batch_size, num_features]\n",
    "        return self.embeddings(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccbd75",
   "metadata": {},
   "source": [
    "## 3. Piecewise Linear Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f9340",
   "metadata": {},
   "source": [
    "Для кодирования вещественных признаков будем использовать Piecewise Linear Encoding от Yandex Research: [On Embeddings for Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).\n",
    "\n",
    "GitHub: https://github.com/yandex-research/rtdl-num-embeddings.\n",
    "\n",
    "<div style=\"width:90%; margin: auto;\">\n",
    "\n",
    "![](https://i.ibb.co/XZtk6fSN/picewise-linear.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "Эмбеддинги числовых признаков формализуются как $z_i = f_i(x_i^{(num)}) \\in \\mathbb{R}^{d_i}$, где:\n",
    "- $f_i(x)$ — функция эмбеддинга для i-го числового признака\n",
    "- $z_i$ — результирующий вектор эмбеддинга\n",
    "- $d_i$ — размерность эмбеддинга\n",
    "\n",
    "Ключевые особенности:\n",
    "- Эмбеддинги вычисляются независимо для каждого признака\n",
    "- В MLP-архитектурах эмбеддинги конкатенируются в один вектор\n",
    "- В Transformer-архитектурах эмбеддинги используются без дополнительных преобразований\n",
    "\n",
    "**Кусочно-линейное кодирование (PLE)**\n",
    "\n",
    "PLE разбивает диапазон значений числового признака на $T$ интервалов (бинов) $B_1, \\ldots, B_T$ с границами $[b_0, b_1], [b_1, b_2], ..., [b_{T-1}, b_T]$.\n",
    "\n",
    "Формальное определение: $\\text{PLE}(x) = [e_1, \\ldots, e_T] \\in \\mathbb{R}^T$\n",
    "\n",
    "где компоненты $e_t$ вычисляются как:\n",
    "\n",
    "$$\n",
    "e_t = \n",
    "\\begin{cases}\n",
    "0, & \\text{если } x < b_{t - 1} \\text{ И } t > 1 \\\\\n",
    "1, & \\text{если } x \\geq b_t \\text{ И } t < T \\\\\n",
    "\\frac{x-b_{t-1}}{b_t-b_{t-1}}, & \\text{иначе}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Важные свойства:\n",
    "\n",
    "- При $T = 1$ PLE эквивалентно скалярному представлению\n",
    "- В отличие от категориальных признаков, PLE учитывает упорядоченность числовых данных\n",
    "- PLE можно рассматривать как предобработку признаков\n",
    "\n",
    "Применение в моделях с вниманием:\n",
    "\n",
    "В моделях с механизмом внимания требуется дополнительно учитывать информацию об индексах признаков:\n",
    "\n",
    "1. Для каждого бина $B_t$ выделяется обучаемый эмбеддинг $v_t \\in \\mathbb{R}^d$\n",
    "2. Итоговый эмбеддинг вычисляется как: $f_i(x) = v_0 + \\sum_{t=1}^T e_t \\cdot v_t = \\text{Linear}(\\text{PLE}(x))$\n",
    "\n",
    "Построение бинов:\n",
    "\n",
    "Наиболее распространенный подход к построению бинов — разбиение по квантилям эмпирического распределения числового признака. Формально:\n",
    "- Для i-го признака: $b_t = q_t \\left(\\{x_j^{(num)}\\}_{j \\in J_{train}}\\right)$, где $q$ — функция эмпирического квантиля\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d68a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseLinearEncodingTransform:\n",
    "    \"\"\"\n",
    "    Applys transformation to training sample\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def compute_bins(\n",
    "        X: torch.Tensor,\n",
    "        n_bins: int,\n",
    "    ) -> list[torch.Tensor]:\n",
    "        bins = [\n",
    "            q.unique()\n",
    "            for q in torch.quantile(\n",
    "                X, torch.linspace(0.0, 1.0, n_bins + 1).to(X), dim=0\n",
    "            ).T\n",
    "        ]\n",
    "        return bins\n",
    "\n",
    "    def __init__(self, dense_train_df, n_bins=32, train_df_slice: int = 1_000_000, name='dense'):\n",
    "        self._name = name\n",
    "        self._bins = PiecewiseLinearEncodingTransform.compute_bins(\n",
    "            dense_train_df.to_torch()[:train_df_slice], n_bins\n",
    "        )\n",
    "        n_features = len(self._bins)\n",
    "        self._n_bins = [len(x) - 1 for x in self._bins]\n",
    "\n",
    "        for n_bin in self._n_bins:\n",
    "            assert n_bin >= 1, \"There is a column with only one unique value\"\n",
    "\n",
    "        single_bin_mask = torch.tensor(self._n_bins) == 1\n",
    "        self.single_bin_mask = single_bin_mask if single_bin_mask.any() else None\n",
    "\n",
    "        max_n_bins = max(self._n_bins)\n",
    "\n",
    "        self.mask = (\n",
    "            None if all(len(x) == len(self._bins[0]) for x in self._bins)\n",
    "            else torch.row_stack(\n",
    "                [\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            torch.ones((len(x) - 1) - 1, dtype=torch.bool),\n",
    "                            torch.zeros(max_n_bins - (len(x) - 1), dtype=torch.bool),\n",
    "                            torch.ones(1, dtype=torch.bool),\n",
    "                        ]\n",
    "                    )\n",
    "                    for x in self._bins\n",
    "                ]\n",
    "            ).flatten(-2)\n",
    "        )\n",
    "\n",
    "        self.weight = torch.zeros(n_features, max_n_bins)\n",
    "        self.bias = torch.zeros(n_features, max_n_bins)\n",
    "\n",
    "        for i, bin_edges in enumerate(self._bins):\n",
    "            bin_width = bin_edges.diff()\n",
    "            w = 1.0 / bin_width\n",
    "            b = -bin_edges[:-1] / bin_width\n",
    "            self.weight[i, -1] = w[-1]\n",
    "            self.bias[i, -1] = b[-1]\n",
    "            self.weight[i, :self._n_bins[i] - 1] = w[:-1]\n",
    "            self.bias[i, :self._n_bins[i] - 1] = b[:-1]\n",
    "\n",
    "    @property\n",
    "    def n_bins(self):\n",
    "        return self._n_bins\n",
    "\n",
    "    def __call__(self, sample: dict[str, tp.Any]) -> dict[str, tp.Any]:\n",
    "        x = sample[self._name].to(torch.float32)\n",
    "        x = torch.addcmul(self.bias, self.weight, x[..., None])\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                x[..., :1].clamp_max(1.0),\n",
    "                x[..., 1:-1].clamp(0.0, 1.0),\n",
    "                (\n",
    "                    x[..., -1:].clamp_min(0.0)\n",
    "                    if self.single_bin_mask is None\n",
    "                    else torch.where(\n",
    "                        self.single_bin_mask[..., None],\n",
    "                        x[..., -1:],\n",
    "                        x[..., -1:].clamp_min(0.0),\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        x = x.flatten(-2)\n",
    "        sample[self._name] = x if self.mask is None else x[:, self.mask]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce23fc",
   "metadata": {},
   "source": [
    "Пример: \n",
    "\n",
    "$weight$ =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{b_1 - b_0} & \\frac{1}{b_2 - b_1} & \\frac{1}{b_3 - b_2} & \\frac{1}{b_4 - b_3} \\\\\n",
    "\\frac{1}{c_1 - c_0} & \\frac{1}{c_2 - c_1} & \\frac{1}{c_3 - c_2} & \\frac{1}{c_4 - c_3}\n",
    "\\end{bmatrix}\n",
    "\n",
    "$bias$ =\n",
    "\n",
    "\\begin{bmatrix}\n",
    "-\\frac{b_0}{b_1 - b_0} & -\\frac{b_1}{b_2 - b_1} & -\\frac{b_2}{b_3 - b_2} & -\\frac{b_3}{b_4 - b_3} \\\\\n",
    "-\\frac{c_0}{c_1 - c_0} & -\\frac{c_1}{c_2 - c_1} & -\\frac{c_2}{c_3 - c_2} & -\\frac{c_3}{c_4 - c_3}\n",
    "\\end{bmatrix}\n",
    "\n",
    "$X$ =\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "\n",
    "$bias + weight \\odot X$ = \n",
    "\\begin{bmatrix}\n",
    "\\frac{x - b_0}{b_1 - b_0} &\n",
    "\\frac{x - b_1}{b_2 - b_1} &\n",
    "\\frac{x - b_2}{b_3 - b_2} &\n",
    "\\frac{x - b_3}{b_4 - b_3}\n",
    "\\\\[8pt]\n",
    "\\frac{y - c_0}{c_1 - c_0} &\n",
    "\\frac{y - c_1}{c_2 - c_1} &\n",
    "\\frac{y - c_2}{c_3 - c_2} &\n",
    "\\frac{y - c_3}{c_4 - c_3}\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f4846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 27])\n",
      "torch.Size([16, 692])\n"
     ]
    }
   ],
   "source": [
    "N_BINS = 32\n",
    "batch_size = 16\n",
    "\n",
    "transform = PiecewiseLinearEncodingTransform(\n",
    "    df_train[YambdaDatasetUtils.NUM_COLS],\n",
    "    n_bins=N_BINS,\n",
    "    name='dense'\n",
    ")\n",
    "input = {\n",
    "    \"label\": torch.ones((batch_size,)),\n",
    "    \"dense\": torch.randn((batch_size, len(YambdaDatasetUtils.NUM_COLS))),\n",
    "    \"sparse\": torch.arange(len(YambdaDatasetUtils.CAT_COLS)).unsqueeze(0).repeat(batch_size, 1)\n",
    "}\n",
    "print(input[\"dense\"].shape)\n",
    "output = transform(input)\n",
    "print(output[\"dense\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f015195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseLinearEncoding(nn.Identity):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3e3a0",
   "metadata": {},
   "source": [
    "## 4. DCN v2 - deep cross network\n",
    "\n",
    "Для агрегации категориальных и вещественных признаков в один скаляр будем использовать DCNv2 от Google DeepMind: [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/abs/2008.13535)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21d897",
   "metadata": {},
   "source": [
    "### Подход\n",
    "\n",
    "<div style=\"width:50%; margin: auto;\">\n",
    "\n",
    "![](https://i.ibb.co/ZqfF5yf/dcn-v2.png)\n",
    "![](https://i.ibb.co/SDYWNSMy/dcn-v2-equation.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "Stacked вариант:\n",
    "- Сначала последовательность кросс-слоев: $$x_{i+1} = x_0 \\odot (W \\times x_i + b) + x_i.$$\n",
    "\n",
    "- Затем последовательность Deep слоев: $$h_{l+1} = f(W_lh_l + b_l).$$\n",
    "\n",
    "\n",
    "### Почему работает и зачем\n",
    "- Знаем, что cross признаки важны. \n",
    "- DNN выучивает только неявные взаимодейтсвия, плохо аппроксимирует dot-product => нужные глубокие сети. \n",
    "- CrossNet добавляет явные взаимодействия признаков => не нужны глубоки DNN => может применять в рантайме.\n",
    "- Явное взаимодействие := $x_1 x_2 \\dots x_d$.\n",
    "\n",
    "<div style=\"width:50%; margin: auto;\">\n",
    "\n",
    "![](https://i.ibb.co/SDWc6y1L/dcn-theory-1.png)\n",
    "![](https://i.ibb.co/qLTgPJYF/dcn-theory-2.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38725746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossLayer(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x0, xl):\n",
    "        return x0 * self.linear(xl) + xl\n",
    "\n",
    "\n",
    "class CrossNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        xl = x\n",
    "        for layer in self.layers:\n",
    "            xl = layer(x, xl)\n",
    "        return xl\n",
    "\n",
    "\n",
    "class DeepNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(input_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = units\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class DCNV2(nn.Module):\n",
    "    def __init__(self, embedding_size, cross_layers, deep_units, input_size, cardinality=65536):\n",
    "        super().__init__()\n",
    "        self.sparse_encode_layer = UnifiedEmbeddings(cardinality, embedding_size)\n",
    "        self.dense_encode_layer = PiecewiseLinearEncoding()\n",
    "        self.cross_network = CrossNetwork(input_size, cross_layers)\n",
    "        self.deep_network = DeepNetwork(input_size, deep_units)\n",
    "        self.output_layer = nn.Linear(deep_units[-1], 1)\n",
    "\n",
    "    def forward(self, dense_input, sparse_input):\n",
    "        sparse_embeddings = self.sparse_encode_layer(sparse_input).view(sparse_input.size(0), -1)\n",
    "        dense_embeddings = self.dense_encode_layer(dense_input)\n",
    "        combined_input = torch.cat([dense_embeddings, sparse_embeddings], dim=-1)\n",
    "        cross_output = self.cross_network(combined_input)\n",
    "        deep_output = self.deep_network(cross_output)\n",
    "        return self.output_layer(deep_output).squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc57aa7",
   "metadata": {},
   "source": [
    "## 5. Обучаем нейросетевое ранжирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a63dbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YambdaDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pl.DataFrame,\n",
    "            transforms: list[tp.Callable[[tp.Any], tp.Any]] | None = None,\n",
    "            batch_size: int = 4096\n",
    "    ):\n",
    "        self._batch_size = batch_size\n",
    "        self._labels_raw = df[YambdaDatasetUtils.LABEL_COl].to_torch().to(torch.float32)\n",
    "        self._dense_raw = df[YambdaDatasetUtils.NUM_COLS].to_torch()\n",
    "        self._sparse_raw = df[YambdaDatasetUtils.CAT_COLS].to_torch()\n",
    "        self._transforms = (\n",
    "            transforms if transforms is not None else []\n",
    "        )\n",
    "\n",
    "        self._labels = []\n",
    "        self._dense = []\n",
    "        self._sparse = []\n",
    "\n",
    "        # precompute all transforms in bathes\n",
    "        for i in tqdm(range(self._labels_raw.size(0) // batch_size)):\n",
    "            sample = {\n",
    "                'label': self._labels_raw[i * batch_size: (i + 1) * batch_size],\n",
    "                'dense_features': self._dense_raw[i * batch_size: (i + 1) * batch_size],\n",
    "                'sparse_features': self._sparse_raw[i * batch_size: (i + 1) * batch_size]\n",
    "            }\n",
    "\n",
    "            for transform in self._transforms:\n",
    "                sample = transform(sample)\n",
    "\n",
    "            self._labels.append(sample['label'])\n",
    "            self._dense.append(sample['dense_features'])\n",
    "            self._sparse.append(sample['sparse_features'])\n",
    "\n",
    "        self._labels = torch.cat(self._labels, dim=0)\n",
    "        self._dense = torch.cat(self._dense, dim=0)\n",
    "        self._sparse = torch.cat(self._sparse, dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._labels.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'label': self._labels[idx],\n",
    "            'dense_features': self._dense[idx],\n",
    "            'sparse_features': self._sparse[idx]\n",
    "        }\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1db172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "cardinality = 8 * 65536\n",
    "seeds = [[2342 + 13 * i, 7777 + 17 * i, 131 + 833 * i] for i in range(len(YambdaDatasetUtils.CAT_COLS))]\n",
    "num_hashes = 3\n",
    "embedding_size = 64\n",
    "n_bins = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcd606eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4787/4787 [02:18<00:00, 34.62it/s]\n",
      "100%|██████████| 696/696 [01:10<00:00,  9.89it/s]\n"
     ]
    }
   ],
   "source": [
    "dense_transform = PiecewiseLinearEncodingTransform(df_train[YambdaDatasetUtils.NUM_COLS], n_bins, name='dense_features')\n",
    "sparse_transform = MultihashTransform(cardinality, seeds, name='sparse_features')\n",
    "transforms = [dense_transform, sparse_transform]\n",
    "\n",
    "train_dataset = YambdaDataset(df_train, transforms)\n",
    "val_dataset = YambdaDataset(df_test, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "953b012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0539e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            int_features, cat_features, labels = batch['dense_features'].to(device), batch['sparse_features'].to(device), batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(int_features, cat_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        all_scores, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader):\n",
    "                int_features, cat_features, labels = batch['dense_features'].to(device), batch['sparse_features'].to(device), batch['label'].to(device)\n",
    "\n",
    "\n",
    "                outputs = model(int_features, cat_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_scores.append(outputs.clone().cpu())\n",
    "                all_labels.append(labels.clone().cpu())\n",
    "            all_scores = torch.cat(all_scores, dim=-1)\n",
    "            all_labels = torch.cat(all_labels, dim=-1)\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "              f'Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100*correct/total:.2f}%, '\n",
    "              f'Val ROC AUC: {roc_auc_score(all_labels, all_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac4ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213\n"
     ]
    }
   ],
   "source": [
    "input_size = 829 + num_hashes * embedding_size * len(YambdaDatasetUtils.CAT_COLS)\n",
    "print(input_size)\n",
    "model = DCNV2(\n",
    "    embedding_size=embedding_size,\n",
    "    cross_layers=3,\n",
    "    deep_units=[1024, 1024, 1024],\n",
    "    input_size=input_size,\n",
    "    cardinality=cardinality,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94276b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4787/4787 [02:47<00:00, 28.66it/s]\n",
      "100%|██████████| 696/696 [00:28<00:00, 24.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 0.3401, Val Loss: 0.3480, Accuracy: 83.19%, Val ROC AUC: 0.9183831665741273\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=1, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090c1a3",
   "metadata": {},
   "source": [
    "## 6. Сравниваем с катбустом на том же наборе признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb3885f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[YambdaDatasetUtils.NUM_COLS + YambdaDatasetUtils.CAT_COLS].to_pandas()\n",
    "y_train = df_train[YambdaDatasetUtils.LABEL_COl].to_pandas()\n",
    "X_test = df_test[YambdaDatasetUtils.NUM_COLS + YambdaDatasetUtils.CAT_COLS].to_pandas()\n",
    "y_test = df_test[YambdaDatasetUtils.LABEL_COl].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96ad61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8755186\tbest: 0.8755186 (0)\ttotal: 282ms\tremaining: 9m 24s\n",
      "100:\ttest: 0.9056464\tbest: 0.9056604 (99)\ttotal: 28.1s\tremaining: 8m 49s\n",
      "200:\ttest: 0.9092687\tbest: 0.9092687 (200)\ttotal: 57.4s\tremaining: 8m 34s\n",
      "300:\ttest: 0.9106317\tbest: 0.9108937 (289)\ttotal: 1m 26s\tremaining: 8m 10s\n",
      "400:\ttest: 0.9115660\tbest: 0.9115660 (400)\ttotal: 1m 56s\tremaining: 7m 44s\n",
      "500:\ttest: 0.9120677\tbest: 0.9120677 (500)\ttotal: 2m 25s\tremaining: 7m 15s\n",
      "600:\ttest: 0.9126255\tbest: 0.9126255 (600)\ttotal: 2m 55s\tremaining: 6m 49s\n",
      "700:\ttest: 0.9131910\tbest: 0.9132269 (691)\ttotal: 3m 26s\tremaining: 6m 21s\n",
      "bestTest = 0.913253963\n",
      "bestIteration = 708\n",
      "Shrink model to first 709 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f3da6d23cb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "\n",
    "train_pool = cb.Pool(X_train, y_train, cat_features=YambdaDatasetUtils.CAT_COLS)\n",
    "test_pool = cb.Pool(X_test, y_test, cat_features=YambdaDatasetUtils.CAT_COLS)\n",
    "\n",
    "model = cb.CatBoostClassifier(\n",
    "    iterations=2000,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    early_stopping_rounds=50,\n",
    "    task_type=\"GPU\",\n",
    "    devices='7',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    ")\n",
    "model.fit(train_pool, eval_set=test_pool, use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa7996a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Feature Id  Importances\n",
      "0           session_hist_duration_seconds_before    17.152574\n",
      "1                  user_item_hist_last_ts_before    12.656227\n",
      "2           user_item_hist_track_finished_before    11.372442\n",
      "3                                            uid    10.912499\n",
      "4         user_item_hist_avg_played_ratio_before     9.722466\n",
      "5              session_hist_skip_fraction_before     8.220881\n",
      "6                                        item_id     7.664724\n",
      "7        session_hist_played_time_seconds_before     4.469543\n",
      "8           user_hist_radio_skip_fraction_before     3.317510\n",
      "9              item_hist_avg_played_ratio_before     2.492329\n",
      "10  user_item_hist_time_since_last_event_seconds     2.415917\n",
      "11                      session_hist_skip_before     2.412685\n",
      "12                    session_hist_events_before     1.695017\n",
      "13                     session_hist_plays_before     0.897447\n",
      "14       user_item_hist_time_span_seconds_before     0.843303\n",
      "15                user_hist_skip_fraction_before     0.793208\n",
      "16               item_hist_track_finished_before     0.562777\n",
      "17                   user_item_hist_plays_before     0.437869\n",
      "18                    user_item_hist_skip_before     0.413512\n",
      "19                    user_item_hist_like_before     0.275367\n",
      "20                         user_hist_like_before     0.237234\n",
      "21               user_hist_track_finished_before     0.207812\n",
      "22                   user_hist_radio_skip_before     0.191532\n",
      "23                      user_hist_last_ts_before     0.175374\n",
      "24                                     timestamp     0.169564\n",
      "25                              session_start_ts     0.113199\n",
      "26          user_hist_radio_play_fraction_before     0.068590\n",
      "27               user_hist_skip_frequency_before     0.055151\n",
      "28                         user_hist_skip_before     0.053245\n"
     ]
    }
   ],
   "source": [
    "importances = model.get_feature_importance(prettified=True)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470a65c",
   "metadata": {},
   "source": [
    "У нейросети получить feature importance не так просто, но есть способы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2f1ae",
   "metadata": {},
   "source": [
    "## 7. Улучшаем нейросетевое ранжирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b819448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O data/album_item_mapping.parquet https://huggingface.co/datasets/yandex/yambda/resolve/main/album_item_mapping.parquet\n",
    "# !wget -O data/artist_item_mapping.parquet https://huggingface.co/datasets/yandex/yambda/resolve/main/artist_item_mapping.parquet\n",
    "# !wget -O data/embeddings.parquet https://huggingface.co/datasets/yandex/yambda/blob/main/embeddings.parquet\n",
    "# !wget -O data/multi_event.parquet https://huggingface.co/datasets/yandex/yambda/resolve/main/flat/50m/multi_event.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c7fb4",
   "metadata": {},
   "source": [
    "- Добавить artist_id, album_id. Это множественные категориальные признаки (список из категориальных признаков). \n",
    "- Добавить контентный эмбеддинг трека. \n",
    "- И еще кучу всего."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
