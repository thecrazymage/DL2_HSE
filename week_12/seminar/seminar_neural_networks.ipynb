{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436319bd",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "Graph Neural Networks (GNNs) are currently the most popular approach to machine learning on graphs. Many GNN architectures can be unified by the Message-Passing Neural Networks (MPNNs) framework. Below we will describe (a variant of) this framework and implement and train several examples of MPNNs.\n",
    "\n",
    "First, let's introduce the notation we will be using in this notebook. Let $G = (V, E)$ be a simple undirected graph without self-loops with node set $V$ and edge set $E$, $|V| = n$, $|E| = m$. Sometimes it will also be handy to use the set $E_{dir}$ of directed edges, $|E_{dir}| = 2m$. Let $N(v)$ be the set of one-hop neighbors of node $v$, and $deg(v)$ be the degree of node $v$, $deg(v) = |N(v)|$. Let $A$ be the adjacency matrix of graph $G$ and $D$ be the diagonal degree matrix of graph $G$, i.e., $D = diag \\Big( deg(v_1), \\; deg(v_2), \\; ..., \\; deg(v_n) \\Big)$.\n",
    "\n",
    "In each layer $l$ an MPNN creates a representation $x_i^l$ of each node $v_i$ from it's previous-layer representation $x_i^{l-1}$ and previous-layer representations of its neighbors. The formula for this transformation at layer $l+1$ is:\n",
    "\n",
    "$$ x_i^{l+1} = \\mathrm{Update} \\Bigg( x_i^l, \\; \\mathrm{Aggregate} \\Big( \\Big\\{ (x_i^l, \\; x_j^l): \\; v_j \\in N(v_i) \\Big\\} \\Big) \\Bigg) $$\n",
    "\n",
    "Here, $\\mathrm{Aggregate}$ is a function that aggregates information from the set of neighbors (since it operates on a set, it should be invariant to the order of neighbors) and $\\mathrm{Update}$ is a function that combines the node's previous-layer representation with the aggregated information from its neighbors. For example, $\\mathrm{Aggregate}$ can be the elementwise mean operation over the set of neighbors and $\\mathrm{Update}$ can be an MLP that takes two concatenated vectors as input:\n",
    "\n",
    "$$ x_i^{l+1} = \\mathrm{MLP} \\Bigg( \\bigg[ x_i^l \\; \\mathbin\\Vert \\; \\mathrm{mean} \\Big( \\Big\\{ x_j^l: \\; v_j \\in N(v_i) \\Big\\} \\Big) \\bigg] \\Bigg) $$\n",
    "\n",
    "(this is actually the first GNN that we will implement in this seminar).\n",
    "\n",
    "The $\\mathrm{Aggregate}$ operation is often called message passing, neighborhood aggregation, or graph convolution. Sometimes this operation is split into $\\mathrm{Message}$ and $\\mathrm{Reduce}$ functions.\n",
    "\n",
    "Note that variations of the above MPNN formula are possible. For example, edge representations can be added, but we won't do it in this seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72da67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f6034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bc10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc361f",
   "metadata": {},
   "source": [
    "Now, let's get us a graph. PyTorch Geometric library provides a lot of popular graph datasets. We will use the Amazon-Computers dataset. It is a co-purchasing network where nodes represent products, edges indicate that two products are frequently bought together, node features are bag-of-words-encoded product reviews, and node labels are product categories. Note that this graph has multiple connected components and some isolated nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73406c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch_geometric==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab958983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd0e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 13752\n",
      "Number of edges: 245861\n",
      "Average node degree: 35.76\n",
      "Number of classes: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = datasets.Amazon(name='computers', root='data')[0]\n",
    "features = data.x\n",
    "labels = data.y\n",
    "edges = data.edge_index.T\n",
    "\n",
    "# The graph is undirected, but is stored as a directed one (like all graphs in PyTorch Geometric),\n",
    "# so each edge appears twice.\n",
    "print(f'Number of nodes: {len(labels)}')\n",
    "print(f'Number of edges: {len(edges) // 2}')\n",
    "print(f'Average node degree: {len(edges) / len(labels):.2f}')\n",
    "print(f'Number of classes: {len(labels.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40cf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_idx = np.arange(len(labels))\n",
    "train_idx, val_and_test_idx = train_test_split(full_idx, test_size=0.5, random_state=0,\n",
    "                                               stratify=labels)\n",
    "\n",
    "val_idx, test_idx = train_test_split(val_and_test_idx, test_size=0.5, random_state=0,\n",
    "                                     stratify=labels[val_and_test_idx])\n",
    "\n",
    "train_idx = torch.from_numpy(train_idx)\n",
    "val_idx = torch.from_numpy(val_idx)\n",
    "test_idx = torch.from_numpy(test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d27aa1",
   "metadata": {},
   "source": [
    "Let's prepare a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2f2bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, optimizer, scaler, amp, graph, features, labels, train_idx):\n",
    "    model.train()\n",
    "\n",
    "    with autocast(enabled=amp):\n",
    "        logits = model(graph=graph, x=features).squeeze(1)\n",
    "        loss = loss_fn(input=logits[train_idx], target=labels[train_idx])\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, metric, amp, graph, features, labels, train_idx, test_idx, val_idx):\n",
    "    model.eval()\n",
    "\n",
    "    with autocast(enabled=amp):\n",
    "        logits = model(graph=graph, x=features)\n",
    "\n",
    "    if metric == 'ROC AUC':\n",
    "        labels = labels.cpu().numpy()\n",
    "        logits = logits.cpu().numpy()\n",
    "        train_idx = train_idx.cpu().numpy()\n",
    "        val_idx = val_idx.cpu().numpy()\n",
    "        test_idx = test_idx.cpu().numpy()\n",
    "        \n",
    "        train_metric = roc_auc_score(y_true=labels[train_idx], y_score=logits[train_idx]).item()\n",
    "        val_metric = roc_auc_score(y_true=labels[val_idx], y_score=logits[val_idx]).item()\n",
    "        test_metric = roc_auc_score(y_true=labels[test_idx], y_score=logits[test_idx]).item()\n",
    "        \n",
    "    elif metric == 'accuracy':\n",
    "        preds = logits.argmax(axis=1)\n",
    "        \n",
    "        train_metric = (preds[train_idx] == labels[train_idx]).float().mean().item()\n",
    "        val_metric = (preds[val_idx] == labels[val_idx]).float().mean().item()\n",
    "        test_metric = (preds[test_idx] == labels[test_idx]).float().mean().item()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'Unknown metric: {metric}.')\n",
    "    \n",
    "    metrics = {\n",
    "        f'train {metric}': train_metric,\n",
    "        f'val {metric}': val_metric,\n",
    "        f'test {metric}': test_metric\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_experiment(graph, features, labels, train_idx, val_idx, test_idx, graph_conv_module, num_layers=2,\n",
    "                   hidden_dim=256, num_heads=4, dropout=0.2, lr=3e-5, num_steps=500, device='cuda:0', amp=False):\n",
    "    num_classes = len(labels.unique())\n",
    "    loss_fn = F.binary_cross_entropy_with_logits if num_classes == 2 else F.cross_entropy\n",
    "    metric = 'ROC AUC' if num_classes == 2 else 'accuracy'\n",
    "    if num_classes == 2:\n",
    "        labels = labels.float()\n",
    "    \n",
    "    model = Model(graph_conv_module=graph_conv_module,\n",
    "                  num_layers=num_layers,\n",
    "                  input_dim=features.shape[1],\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  output_dim=1 if num_classes == 2 else num_classes,\n",
    "                  num_heads=num_heads,\n",
    "                  dropout=dropout)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler(enabled=amp)\n",
    "    \n",
    "    graph = graph.to(device)\n",
    "    features = features.to(device)\n",
    "    labels = labels.to(device)\n",
    "    train_idx = train_idx.to(device)\n",
    "    val_idx = val_idx.to(device)\n",
    "    test_idx = test_idx.to(device)\n",
    "    \n",
    "    best_val_metric = 0\n",
    "    corresponding_test_metric = 0\n",
    "    best_step = None\n",
    "    with tqdm(total=num_steps) as progress_bar:\n",
    "        for step in range(1, num_steps + 1):\n",
    "            train_step(model=model, loss_fn=loss_fn, optimizer=optimizer, scaler=scaler, amp=amp, graph=graph,\n",
    "                       features=features, labels=labels, train_idx=train_idx)\n",
    "            metrics = evaluate(model=model, metric=metric, amp=amp, graph=graph, features=features, labels=labels,\n",
    "                               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx)\n",
    "\n",
    "            progress_bar.update()\n",
    "            progress_bar.set_postfix({metric: f'{value:.2f}' for metric, value in metrics.items()})\n",
    "            \n",
    "            if metrics[f'val {metric}'] > best_val_metric:\n",
    "                best_val_metric = metrics[f'val {metric}']\n",
    "                corresponding_test_metric = metrics[f'test {metric}']\n",
    "                best_step = step\n",
    "    \n",
    "    print(f'Best val {metric}: {best_val_metric:.4f}')\n",
    "    print(f'Corresponding test {metric}: {corresponding_test_metric:.4f}')\n",
    "    print(f'(step {best_step})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add8f70",
   "metadata": {},
   "source": [
    "This should look quite similar to your standard training loop, but with one notable difference - there are no mini-batches, we are always training on the whole graph. Since the data samples (graph nodes) are not independent, we cannot trivially sample a mini-batch.\n",
    "\n",
    "Now, let's implement a model. Don't forget about skip connections and layer normalization - they can signififcantly boost the performance of a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, dim, num_inputs, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features=num_inputs * dim, out_features=dim)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualModule(nn.Module):\n",
    "    def __init__(self, graph_conv_module, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(normalized_shape=dim)\n",
    "        self.graph_conv = graph_conv_module(dim=dim, num_heads=num_heads)\n",
    "        self.feed_forward = FeedForwardModule(dim=dim, num_inputs=2, dropout=dropout)\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        x_res = self.normalization(x)\n",
    "        \n",
    "        x_aggregated = self.graph_conv(graph, x_res) # <---- \n",
    "        x_res = torch.cat([x_res, x_aggregated], axis=1)\n",
    "        \n",
    "        x_res = self.feed_forward(x_res)\n",
    "        \n",
    "        x = x + x_res\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, graph_conv_module, num_layers, input_dim, hidden_dim, output_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.input_linear = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.input_dropout = nn.Dropout(p=dropout)\n",
    "        self.input_act = nn.GELU()\n",
    "        \n",
    "        self.residual_modules = nn.ModuleList(\n",
    "            ResidualModule(graph_conv_module=graph_conv_module, dim=hidden_dim, num_heads=num_heads,\n",
    "                           dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        )\n",
    "        \n",
    "        self.output_normalization = nn.LayerNorm(hidden_dim)\n",
    "        self.output_linear = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = self.input_dropout(x)\n",
    "        x = self.input_act(x)\n",
    "        \n",
    "        for residual_module in self.residual_modules:\n",
    "            x = residual_module(graph, x)\n",
    "        \n",
    "        x = self.output_normalization(x)\n",
    "        logits = self.output_linear(x)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030a0f4",
   "metadata": {},
   "source": [
    "Now everything is ready - except for the graph convolution module. We will implement several variants of this module, which will constitute the only difference between our GNNs. But first - as a simple baseline - let's implement a graph convolution module that does nothing. It will allow us to see how a graph-agnostic model performs, so we can then compare our GNNs to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2554025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        return torch.zeros_like(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9f0be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2837301/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767c2694f3a2419ba8587ff7fc4f89cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2837301/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2837301/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.8464\n",
      "Corresponding test accuracy: 0.8319\n",
      "(step 459)\n"
     ]
    }
   ],
   "source": [
    "graph = torch.empty(0)   # We don't care about graph representation for this experiment.\n",
    "\n",
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DummyGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11368a",
   "metadata": {},
   "source": [
    "Now let's implement some real graph convolutions. Simple graph convolutions can be represented as operations with (sparse) matrices. Thus, they can be implemented in pure PyTorch. We will need the graph adjacency matrix $A$, the graph degree matrix $D$, and the matrix of node representations at layer $l$ $X^l$. Further, let $\\tilde{x_i}^{l}$ be the output of $\\mathrm{Aggregate}$ function at layer $l$ for node $v_i$ and let $\\widetilde{X}^l$ be the matrix of stacked vectors $\\tilde{x_i}^{l}$ for all nodes.\n",
    "\n",
    "For the next couple experiments, assume that the graph argument of the graph convolution forward method is a sparse adjacency matrix of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d209599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 491722])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b76cf686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13752"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb74d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 13751, 13751, 13751],\n",
       "                       [  507,  6551,  8210,  ..., 12751, 13019, 13121]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(13752, 13752), nnz=491722, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = torch.sparse_coo_tensor(\n",
    "    indices=edges.T,\n",
    "    values=torch.ones(len(edges)),\n",
    "    size=(len(labels), len(labels))\n",
    ")\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c600c",
   "metadata": {},
   "source": [
    "Let's implement a graph convolution that simply takes the mean of neighbors' representations. We can write:\n",
    "\n",
    "$$ \\tilde{x}_i^{l+1} = \\frac{1}{|N(v_i)|} \\sum_{v_j \\in N(v_i)} x_j^l $$\n",
    "\n",
    "This operation can be written in matrix form:\n",
    "\n",
    "$$ \\widetilde{X}^{l+1} = D^{-1} A X^l $$\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f10dac",
   "metadata": {},
   "source": [
    "Additionally, we can fuse the computation of inverse degree matrix and speed up the execution even more! However, this optimization doesn't work witn inductive setting, where a graph changes during the inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "        A = graph\n",
    "        X = x\n",
    "    \n",
    "        if self.cache is None:\n",
    "            # compute node degrees:\n",
    "            degrees_sparse = graph.sum(axis=0)\n",
    "\n",
    "            # construct sparse tensor with diagonal values populated by inverse node degrees D^{-1}:\n",
    "            degrees_sparse_indices = degrees_sparse.indices().squeeze()\n",
    "            degrees_sparse_values = degrees_sparse.values()\n",
    "            degrees_sparse_values[degrees_sparse_values == 0] = 1\n",
    "            diagonal_indices = torch.stack([degrees_sparse_indices, degrees_sparse_indices])\n",
    "\n",
    "            inverse_degree_sparse_tensor = torch.sparse_coo_tensor(\n",
    "                indices=diagonal_indices,\n",
    "                values=1 / degrees_sparse_values,\n",
    "                size=(len(graph), len(graph))\n",
    "            )\n",
    "\n",
    "            # compute D^{-1} @ A and cache this fused matrix\n",
    "            self.cache = inverse_degree_sparse_tensor @ A\n",
    "\n",
    "\n",
    "        # x_aggregated = (A @ X) / d\n",
    "        x_aggregated = self.cache @ X\n",
    "    \n",
    "        ######################\n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d71b93",
   "metadata": {},
   "source": [
    "(The computations can be sped up by precomputing $D^{-1} A$, but we won't do it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09e41892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2837301/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3c8e14d0fc4927ab7d704a6923e519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2837301/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2837301/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.9040\n",
      "Corresponding test accuracy: 0.8973\n",
      "(step 488)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=MeanGraphConv,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefdb56c",
   "metadata": {},
   "source": [
    "As we can see, the accuracy is a lot better than in the previous experiment - our GNN works better than a graph-agnostoc model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698594f2",
   "metadata": {},
   "source": [
    "Now, let's try another simple GNN variant - this time we will implement a graph convolution proposed in [the GCN paper](https://arxiv.org/abs/1609.02907). The formula is:\n",
    "\n",
    "$$ \\tilde{x}_i^{l+1} = \\sum_{v_j \\in N(v_i)} \\frac{1}{\\sqrt{deg(v_i) deg(v_j)}} x_j^l $$\n",
    "\n",
    "It's very similar to the mean convolution, except we normalize each neighbor's representation not by the degree of the ego node, but by the geometric mean of the degree of the ego node and the neighbor. This operation can be written in matrix form:\n",
    "\n",
    "$$ \\widetilde{X}^{l+1} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} X^l $$\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25de997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        A = graph\n",
    "        X = x\n",
    "        d = A.sum(axis=0).to_dense()[:, None]\n",
    "        d[d == 0] = 1   # There are some isolated nodes, and we don't want to divide by zero.\n",
    "        d_inv_sqrt = d.rsqrt()\n",
    "\n",
    "        x_aggregated = (A * d_inv_sqrt * d_inv_sqrt.T) @ X        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01341bf6",
   "metadata": {},
   "source": [
    "(The computations can be sped up by precomputing $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, but we won't do it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40723c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa10867092764610b783a67712fdf4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.9055\n",
      "Corresponding test accuracy: 0.8973\n",
      "(step 434)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=GCNGraphConv,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24658a84",
   "metadata": {},
   "source": [
    "The results are similar to those in the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65be773",
   "metadata": {},
   "source": [
    "Simple graph convolutions can be expressed as matrix operations, and thus, can be implemented in pure PyTorch. However, efficient implementation of more complex graph convolutions requires using specialized libraries. There are two most popular GNN libraries for PyTorch - [PyTorch Geometric (PyG)](https://github.com/pyg-team/pytorch_geometric) and [Deep Graph Library (DGL)](https://www.dgl.ai/). In this seminar, we will be using DGL, because ~it is objectively better~ the instructor likes it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "863c2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9f583f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd4783",
   "metadata": {},
   "source": [
    "There are many features for deep learning on graphs in DGL, but we will only be using two of them - the Graph class, which is obviously used for representing a graph, and the [ops module](https://docs.dgl.ai/api/python/dgl.ops.html), which contains operators for message passing on graphs.\n",
    "\n",
    "First, let's create a graph representation which we will be using in the next few experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b139af27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=13752, num_edges=491722,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(labels))\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7a06e",
   "metadata": {},
   "source": [
    "Now let's reimplement the mean graph convolution, this time using DGL. For this we will need a certain operation from the ops module - can you guess which one by their names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50885ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLMeanGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        x_aggregated = ops.copy_u_mean(graph, x)\n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "484ad64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b625f53cb28439399b032f6163a0e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/home/fvelikon/micromamba/envs/graph_ml/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py:157: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.\n",
      "  return th.cuda.amp.autocast_mode._cast(\n",
      "/home/fvelikon/micromamba/envs/graph_ml/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py:148: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return th.cuda.amp.autocast(enabled=False)\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.9081\n",
      "Corresponding test accuracy: 0.9011\n",
      "(step 455)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLMeanGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06023062",
   "metadata": {},
   "source": [
    "The results are roughly the same as for the pure PyTorch implementation, but the training is faster (graph message passing operations with DGL a generally faster than PyTorch sparse matrix multiplications, and, further, DGL supports using AMP with most of its operations, while PyTorch does not (yet) allow using AMP with sparse matrix operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578c692",
   "metadata": {},
   "source": [
    "By simply swapping the ops.copy_u_mean function for the ops.copy_u_max function, we can get another graph convolution that computes the elementwise maximum of neighbors' representations. This one cannot be efficiently implemented in pure PyTorch. Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f4af522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLMaxGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        x_aggregated = ops.copy_u_max(graph, x)\n",
    "        x_aggregated[x_aggregated.isinf()] = 0   # There are some isolated nodes, and we do not want -inf.\n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8135b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f5007968ba4179ba8e2f27bb868efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.9107\n",
      "Corresponding test accuracy: 0.9098\n",
      "(step 405)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLMaxGraphConv,\n",
    "               device=device)   # This one currently does not work with AMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a66478",
   "metadata": {},
   "source": [
    "Now, let's reimplement the GCN graph convolution using DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5336ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLGCNGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        d = graph.in_degrees()\n",
    "        d_inv_sqrt = 1 / d.sqrt()\n",
    "        \n",
    "        weights = ops.u_mul_v(graph, d_inv_sqrt, d_inv_sqrt)\n",
    "        \n",
    "        x_aggregated = ops.u_mul_e_sum(graph, x, weights)        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0e78c",
   "metadata": {},
   "source": [
    "(The computations can be sped up by precomputing weights, but we won't do it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e50b006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5504b66e7f4a7ea80407084cf3048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.9072\n",
      "Corresponding test accuracy: 0.8895\n",
      "(step 465)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLGCNGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c623c2c",
   "metadata": {},
   "source": [
    "Now let's implement something more complex - the graph convolution proposed in [the GAT paper](https://arxiv.org/abs/1710.10903). This one uses attention (although a very simple version of it). The formulas are:\n",
    "\n",
    "$$ s_{ij} = \\mathrm{LeakyReLU} \\Big( w_1^T x_i^l + w_2^T x_j^l + b \\Big)\\;\\;\\;\\;\\; \\forall (i, j) \\in E_{dir} $$\n",
    "\n",
    "$$ \\Big( p_{ij}: \\; v_j \\in N(v_i) \\Big) = softmax \\Big( s_{ij}: \\; v_j \\in N(v_i) \\Big) \\;\\;\\;\\;\\; \\forall i = 1, ..., n$$\n",
    "\n",
    "This clunky notation means that for each node we take the softmax of attention scores ($s_{ij}$) of its neighbors to get attention probabilities ($p_{ij}$) corresponding to these neighbors. Another way to write it is:\n",
    "\n",
    "$$ p_{ij} = \\frac{ \\exp{(s_{ij})} }{ \\sum_{v_k \\in N(v_i)} \\exp{(s_{ik})} } \\;\\;\\;\\;\\; \\forall (i, j) \\in E_{dir} $$\n",
    "\n",
    "The necessary edge softmax function is available in DGL.\n",
    "\n",
    "$$ \\tilde{x}_i^{l+1} = \\sum_{v_j \\in N(v_i)} p_{ij} x_j^l $$\n",
    "\n",
    "Note that additionally the attention mechanism is multi-headed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36560d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.functional import edge_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd9bf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLGATGraphConv(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, **kwargs):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "    \n",
    "        self.attn_linear_u = nn.Linear(in_features=dim, out_features=num_heads)\n",
    "        self.attn_linear_v = nn.Linear(in_features=dim, out_features=num_heads, bias=False)\n",
    "        self.attn_act = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        ######################\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        attn_scores_u = self.attn_linear_u(x)\n",
    "        attn_scores_v = self.attn_linear_v(x)\n",
    "\n",
    "        attn_scores = ops.u_add_v(graph, attn_scores_u, attn_scores_v)\n",
    "\n",
    "        attn_scores = self.attn_act(attn_scores)\n",
    "        attn_probs = edge_softmax(graph, attn_scores)\n",
    "        \n",
    "        x = x.reshape(len(x), self.head_dim, self.num_heads)\n",
    "        x_aggregated = ops.u_mul_e_sum(graph, x, attn_probs)\n",
    "        x_aggregated = x_aggregated.reshape(len(x), self.dim)        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a402962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac17ef2dfdd45f3a5d6d0d68adc72b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.9113\n",
      "Corresponding test accuracy: 0.8985\n",
      "(step 424)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLGATGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12199e4d",
   "metadata": {},
   "source": [
    "Now, let's see if GNNs can achieve strong performance on a heterophilous graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9e036ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 24492\n",
      "Number of edges: 93050\n",
      "Average node degree: 7.60\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "data = datasets.HeterophilousGraphDataset(name='amazon-ratings', root='data/heterophilous-graphs')[0]\n",
    "features = data.x\n",
    "labels = data.y\n",
    "edges = data.edge_index.T\n",
    "\n",
    "# The graph is undirected, but is stored as a directed one (like all graphs in PyTorch Geometric),\n",
    "# so each edge appears twice.\n",
    "print(f'Number of nodes: {len(labels)}')\n",
    "print(f'Number of edges: {len(edges) // 2}')\n",
    "print(f'Average node degree: {len(edges) / len(labels):.2f}')\n",
    "print(f'Number of classes: {len(labels.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d27682b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same split as in the previous seminar, except we now use part of the previous seminar's \n",
    "# train set as a val set.\n",
    "train_idx = torch.where(data.train_mask[:, 0])[0]\n",
    "val_idx = torch.where(data.val_mask[:, 0])[0]\n",
    "test_idx = torch.where(data.test_mask[:, 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5122da0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=24492, num_edges=186100,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(labels))\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd83f2",
   "metadata": {},
   "source": [
    "As always, let's first try a graph-agnostic baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c370cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bef26f48cc4d698b6252d8c512e3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.4684\n",
      "Corresponding test accuracy: 0.4761\n",
      "(step 2473)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DummyGraphConv,\n",
    "               device=device, amp=True, num_steps=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8517ba",
   "metadata": {},
   "source": [
    "Let's see if a GNN with mean graph convolution can achieve significantly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a45923d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa5b6f45434e1a84dd539b10e371b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2547007/3959283960.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n",
      "/var/tmp/ipykernel_2547007/3959283960.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy: 0.5314\n",
      "Corresponding test accuracy: 0.5404\n",
      "(step 1828)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLMeanGraphConv,\n",
    "               device=device, amp=True, num_steps=2500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
